# -*- coding: utf-8 -*-
"""
optimal features.py

1. Start with all features selected in script 2.
2. Fit Random Forrest, test on the training set, calculate feature importance
3. Reduce iteratively the feature set with the least important feature, recalibrate the model, check ROC
4. Pick the best set of features to be utilized in the following 4. backtesting.py script
"""
### PACKAGES
# Preprocessing
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns
import json

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import cross_val_score
from sklearn import tree
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import permutation_importance
from sklearn.ensemble import RandomForestClassifier 
from sklearn.metrics import roc_curve, roc_auc_score 
import shap
import statsmodels.api as sm

# Tree visualization
import graphviz
from IPython.display import Image
from sklearn.tree import export_graphviz
# Make sure Graphviz is in the PATH
os.environ["PATH"] += os.pathsep + 'C:/Program Files/Graphviz/bin'
from utils import status_calc

def backtest(shuffle_train_test = True, test_size = 0.2, topNfeat = 15, minNfeat = 6):
    """
    A backtest, which splits the dataset into a train set and test set,
    then fits a Random Forest classifier to the train set. 
    Further remove iteratively the least important feature to check the changes in ROC
    
    shuffle_train_test - true if train/test split should be random
    test_size - test set sample size
    topNfeat - maximum of features in the search
    minNfeat - minimum size of features set
    """
    # Change from scientific view to float
    pd.set_option('display.float_format', lambda x: '%.3f' % x)

    print("***************************************")
    print(f'shuffle_train_test: {shuffle_train_test}')
    print(f'test_size: {test_size}')
    print("***************************************")
    # Opening JSON file
    f = open('INPUT\input_scrap.json')
    # returns JSON object as a dictionary
    data = json.load(f)
    
    # Retrieve input details
    # The percentage by which a stock has to beat the S&P500 to be considered a 'buy'
    RandomForest_no_estimators  = data['RandomForest_no_estimators']
    outperformance_             = data['outperform_idx'] * 100
    
    roc = []
    features_ = []
    # Build the dataset, and drop any rows with missing values
    data_df = pd.read_csv("OUTPUT/keystats_new.csv", index_col = "date")
    data_df.dropna(axis = 0, how = "any", inplace = True)
    data_df = data_df.replace([np.inf, -np.inf], 0)
    # In case of extreme Trailing P/Es, overwrite with 0 (observed in few cases)
    if 'Trailing P/E' in data_df.columns:
        data_df.loc[data_df['Trailing P/E'] > 10000, 'Trailing P/E'] = 0
        data_df.loc[data_df['Trailing P/E'] < -10000, 'Trailing P/E'] = 0
    features = [x for x in data_df.columns if x not in ['Unnamed: 0.1', 'Unnamed: 0', 'Unnamed: 0_QoQpc', 'Unnamed: 0_YoYpc', 'Unnamed: 0.1_QoQpc', 'Unnamed: 0.1_YoYpc',
                                                        'date_ticker','Year', 'Quarter', 'Ticker', 'Close','Price' , 'SP500', 'Close_YoYpc', 'SP500_YoYpc', 'CloseOutIDX', 'CloseOutIDX_QoQpc', 'CloseOutIDX_YoYpc']]
    X = data_df[features].values

    # The labels are generated by applying the status_calc to the dataframe.
    # '1' if a stock beats the S&P500 by more than x%, else '0'. Here x is the
    # outperformance parameter, which is set to 10 by default but can be redefined.
    y = list(status_calc(data_df["Close_YoYpc"] * 100, data_df["SP500_YoYpc"] * 100, outperformance = outperformance_))

    # Stored to track returns
    z = np.array(data_df[["Close_YoYpc", "SP500_YoYpc"]])

    # Generate the train set and test set by splitting the dataset according to date - first 1 - test_size portion goes to training, the latest
    # quarters fall into testing set
    labels = data_df['Quarter'] + "_" + data_df['Ticker']    
    train_quarters = np.unique(data_df['Quarter'])[:int((1 - test_size) * len(np.unique(data_df['Quarter'])))].tolist()
    test_quarters  = np.unique(data_df['Quarter'])[ int((1 - test_size) * len(np.unique(data_df['Quarter']))):].tolist()
    X_train = data_df[data_df['Quarter'].isin(train_quarters)][features].values
    X_test  = data_df[data_df['Quarter'].isin(test_quarters)][features].values
    y_train = y[:len(X_train)]
    y_test  = y[len(X_train):]
    z_train = z[:len(X_train)]
    z_test  = z[len(X_train):]
    labels_train = labels[:len(X_train)]
    labels_test  = labels[len(X_train):]
    # Instantiate a RandomForestClassifier with 100 trees, then fit it to the training data
    clf = RandomForestClassifier(n_estimators = RandomForest_no_estimators, random_state=137, max_depth = 5, min_samples_leaf = 2, min_samples_split = 5)
    clf.fit(X_train, y_train)
    # Generate the predictions, then print test set accuracy and precision
    y_pred = clf.predict(X_test)
    print(classification_report(y_test, y_pred))
    print('*** RANDOM FORREST ***')
    print("Confusion matrix")
    print(f"{confusion_matrix(y_test, y_pred)}")
    print("Classifier performance\n", "=" * 20)
    print(f"Accuracy score: {clf.score(X_test, y_test): .2f}")
    print(f"Precision score: {precision_score(y_test, y_pred): .2f}")
    ## Confusion matrix - Get and reshape confusion matrix data
    matrix = confusion_matrix(y_test, y_pred)
    matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]
    
    # Build the plot
    plt.figure(figsize=(16,7))
    sns.set(font_scale=1.4)
    sns.heatmap(matrix, annot=True, annot_kws={'size':10},
                cmap=plt.cm.Greens, linewidths=0.2)
    
    # Add labels to the plot
    class_names = ['Not outperform', 'Outperform']
    tick_marks = np.arange(len(class_names))
    tick_marks2 = tick_marks + 0.5
    plt.xticks(tick_marks, class_names, rotation=25)
    plt.yticks(tick_marks2, class_names, rotation=0)
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.title(f'Confusion Matrix for Random Forest Model, (shuffle_train_test, test_size): {shuffle_train_test, test_size}')
    plt.show()
    
    ## Feature improtance
    sorted_idx = clf.feature_importances_.argsort()
    perm_importance = permutation_importance(clf, X_test, y_test)
    sorted_idx_perm = perm_importance.importances_mean.argsort()
    
    ## ROC AUC
    # Compute the false positive rate (FPR)  
    # and true positive rate (TPR) for different classification thresholds 
    fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1)
    # Compute the ROC AUC score 
    roc_auc = roc_auc_score(y_test, y_pred) 
    print("ROC AUC")
    print(roc_auc)
    # Plot the ROC curve 
    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc) 
    # roc curve for tpr = fpr  
    plt.plot([0, 1], [0, 1], 'k--', label='Random classifier') 
    plt.xlabel('False Positive Rate') 
    plt.ylabel('True Positive Rate') 
    plt.title("ROC Curve all features")
    plt.legend(loc="lower right") 
    plt.show()
    
    roc.append(roc_auc)
    features_.append(features)
    ##################################################################################
    # Limit set to features with TopN Importance, starting from topNfeat to minNfeat #
    ##################################################################################
    for x in range(0, topNfeat - minNfeat + 1):    
        print("*** LIMITED FEATURES = ", topNfeat - x, " ***")
        no_features = topNfeat - x
        new_features = data_df.columns[8:][sorted_idx][-(no_features):]
        X = data_df[new_features].values
    
        # Generate the train set and test set by randomly splitting the dataset
        # Random state set the same to model init to get reproducible results
        # Generate the train set and test set by splitting the dataset according to date - first 1 - test_size portion goes to training, the latest
        # quarters fall into testing set
        X_train = data_df[data_df['Quarter'].isin(train_quarters)][new_features].values
        X_test  = data_df[data_df['Quarter'].isin(test_quarters)][new_features].values
        
        
        # Instantiate a RandomForestClassifier with 100 trees, then fit it to the training data
        clf = RandomForestClassifier(n_estimators = RandomForest_no_estimators, random_state = 137, max_depth = 5, min_samples_leaf = 2, min_samples_split = 5)
        clf.fit(X_train, y_train)
        # Generate the predictions, then print test set accuracy and precision
        y_pred = clf.predict(X_test)
        print(classification_report(y_test, y_pred))
        print("*** RANDOM FORREST LIMITED FEATURES ", topNfeat - x, " ***")
        print("Confusion matrix")
        print(f"{confusion_matrix(y_test, y_pred)}")
        print("Classifier performance\n", "=" * 20)
        print(f"Accuracy score: {clf.score(X_test, y_test): .2f}")
        print(f"Precision score: {precision_score(y_test, y_pred): .2f}")
        #* 100 CONFUSION MATRIX - Get and reshape confusion matrix data
        matrix = confusion_matrix(y_test, y_pred)
        matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]
        
        # Build the plot
        plt.figure(figsize=(16,7))
        sns.set(font_scale=1.4)
        sns.heatmap(matrix, annot=True, annot_kws={'size':10},
                    cmap=plt.cm.Greens, linewidths=0.2)
        
        # Add labels to the plot
        class_names = ['Not outperform', 'Outperform']
        tick_marks = np.arange(len(class_names))
        tick_marks2 = tick_marks + 0.5
        plt.xticks(tick_marks, class_names, rotation=25)
        plt.yticks(tick_marks2, class_names, rotation=0)
        plt.xlabel('Predicted label')
        plt.ylabel('True label')
        plt.title(f"Confusion Matrix for Random Forest Model, limited features, (shuffle_train_test, test_size, no features): {shuffle_train_test, test_size, topNfeat - x}")
        plt.show()
        
        #* 100 ROC AUC
        # Compute the false positive rate (FPR)  
        # and true positive rate (TPR) for different classification thresholds 
        fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1)
        # Compute the ROC AUC score 
        roc_auc = roc_auc_score(y_test, y_pred) 
        print("ROC AUC LIMITED FEATURES ", topNfeat - x, " ***")
        print(roc_auc)
        # Plot the ROC curve 
        plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc) 
        # roc curve for tpr = fpr  
        plt.plot([0, 1], [0, 1], 'k--', label='Random classifier') 
        plt.xlabel('False Positive Rate') 
        plt.ylabel('True Positive Rate') 
        plt.title(str("ROC Curve " + str(topNfeat - x) + " features"))
        plt.legend(loc="lower right") 
        plt.show()
        
        roc.append(roc_auc)
        features_.append(new_features)
    
    roc = pd.DataFrame(roc)
    a = [str(topNfeat - x) for x in range(0, topNfeat - minNfeat + 1)]
    a.insert(0, str(len(features_[0])))
    roc['No features'] = a
    plt.plot(roc['No features'].tolist(), roc[0].tolist())
    plt.xticks(rotation = 45)
    roc['Features'] = features_
    
    return roc

if __name__ == "__main__":
    roc = backtest()
